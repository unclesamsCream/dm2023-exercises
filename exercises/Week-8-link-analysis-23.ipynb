{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8 - Link Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week, we are going to work with link analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tabulate\n",
    "import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('./utilities')\n",
    "from utilities.load_data import load_dblp_citations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we have included some code to plot the karate graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_karate(G, pr=[], ax=None, cmap=plt.get_cmap('RdYlBu')): \n",
    "    fixed_positions = {0:(10.74,4.07),1:(9.76,6.48),2:(8.39,5.21),3:(10.37,1.98),4:(12.30,5.61),5:(13.31,3.28),6:(13.28,5.00),7:(8.41,7.06),8:(6.72,4.31),9:(5.77,1.38),10:(12.30,2.72),11:(12.75,4.05),12:(11.32,2.41),13:(8.70,2.88),14:(3.33,0.63),15:(1.88,2.01),16:(13.92,4.05),17:(10.77,5.61),18:(0.69,6.40),19:(9.05,1.38),20:(0.34,4.63),21:(11.56,6.22),22:(5.24,0.34),23:(1.88,7.49),24:(5.11,6.80),25:(4.31,8.52),26:(2.14,0.32),27:(3.65,6.64),28:(6.03,5.24),29:(0.77,2.91),30:(7.01,2.43),31:(6.61,7.86),32:(4.60,4.52),33:(4.39,2.91)}\n",
    "    if len(pr) :\n",
    "        nx.draw(G, with_labels=True, pos=fixed_positions, ax=ax, cmap=cmap, node_color=pr)\n",
    "    else : \n",
    "        nx.draw(G, with_labels=True, pos=fixed_positions, ax=ax)\n",
    "\n",
    "G = nx.karate_club_graph()\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "magic_cut = np.array([1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, -1, -1, 1, 1, -1, 1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1])\n",
    "\n",
    "plot_karate(G, magic_cut, ax=ax[0])\n",
    "plot_karate(G, np.random.rand(G.order()), ax=ax[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 - Random walks and PageRank\n",
    "\n",
    "Random walks are tightly connected to the Pagerank as a limit of convergence in performing random walks on a graph. \n",
    "\n",
    "In this theoretical exercise we will reason about PageRank and its properties. Remember that the Personalized Pagerank in a undirected graph can be experssed by the equation \n",
    "$$\n",
    "\\mathbf{r} = \\alpha M\\mathbf{r} - (1-\\alpha)\\mathbf{p} \n",
    "$$\n",
    "where $M = A^\\top \\Delta^{-1}$ is the transition matrix, $\\mathbf{p}$ is called the restart vector and represents the starting nodes of the random walk where the walker will jump back with probability $(1-\\alpha)$. \n",
    "\n",
    "Let us assume $\\alpha = 0.85$ and $\\mathbf{p} = \\frac{1}{n}\\mathbf{1}_n$ is uniformly distributed where $\\mathbf{1}_n$ is the $n$-dimensional vectors with all 1s. \n",
    "\n",
    "1. (Warm up) Argue why $M = A^\\top \\Delta^{-1}$ makes sense as a transition matrix. \n",
    "\n",
    "2. What is the PageRank of a circle of $n$ nodes?\n",
    "\n",
    "3. A [path graph](https://en.wikipedia.org/wiki/Path_graph) is a graph where each node is connected to the consecutive. What is the PageRank with 3 nodes when $\\alpha=1$? \n",
    "**Hint**: Remember that the elements in the PageRank vector sum to 1 (it's a distribution over nodes). \n",
    "\n",
    "4. Consider a complete directed bipartite graph (with self loops) consisting of 3 hubs and 2 authorities:  \n",
    "![hubs_and_authorities](graphics/bipartite_1.png)\n",
    "    1. Write the adjacency matrix of the system, and normalize it for use with the PageRank algorithm.\n",
    "    2. What is the PageRank score of the nodes in the system? Provide both an analytical proof and an intuitive explanation. Assume no damping factor.\n",
    "    3. Now add a link from one of the authorities to one of the hubs. What is the PageRank score of the nodes now? Provide both an analytical proof and an intuitive explanation. \n",
    "\n",
    "\n",
    "5. Prove the following statement:\n",
    "In an undirected and connected graph, $G(V,E)$, the stationary distribution of a random walk is proportional to the nodes' degrees.\n",
    "\n",
    "In other words, for a transition matrix $ M $, in which \n",
    "$$\n",
    "    M(u, v) = \\left\\{ \n",
    "    \\begin{matrix}\n",
    "    \\frac{1}{d_u}&\\text{, if }(u, v) \\in E\\\\ \n",
    "    0&\\text{, otherwise}\n",
    "    \\end{matrix}\n",
    "    \\right.\n",
    "$$\n",
    "and a stationary distribution $\\pi$, such that $\\pi = M\\pi$, prove that probability $\\pi_i$ is proportional to $d_i$, the number of edges incident on note $i$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - Implementing PageRank\n",
    "\n",
    "1. Provide an implementation of PageRank using the Power Iteration method below and run it on the karate graph, and the graphs discussed in exercise 1.\n",
    "2. Check your code below. Why does the method fail for two of the graphs when $\\alpha = 1$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.algorithms import bipartite\n",
    "\n",
    "# Tolerance is the sum of the absolute differences among the pagerank vectors in consequent iterations\n",
    "# The iteration stops is the the differences are less than the tolerance\n",
    "def my_pagerank(G, alpha = 0.85, p = None, max_iter = 100, tol=1e-06) : \n",
    "    A = nx.adjacency_matrix(G)\n",
    "    n = G.number_of_nodes()\n",
    "    r = np.full(n, 1/n)\n",
    "\n",
    "    if p is None : \n",
    "        p = np.full(n, 1/n)\n",
    "\n",
    "    ### YOUR CODE HERE    \n",
    "    ### YOUR CODE HERE\n",
    "    return r\n",
    "\n",
    "\n",
    "# simple helper function to simplify comparing nx pagerank with your own pagerank\n",
    "def change_nx_format(nx_pagerank):\n",
    "    keys = sorted(nx_pagerank.keys())\n",
    "    return np.array([nx_pagerank[i] for i in keys])\n",
    "    \n",
    "    \n",
    "alpha=.85  # Try different values (including 1)\n",
    "\n",
    "G_karate = nx.karate_club_graph()\n",
    "print('Pagerank karate graph')\n",
    "print(\"nx:\\n\", change_nx_format(nx.pagerank_numpy(G_karate, alpha=alpha)))\n",
    "print(\"my:\\n\", my_pagerank(G_karate, alpha=alpha))\n",
    "print()\n",
    "\n",
    "G_circle = nx.circulant_graph(10, [1])\n",
    "print('Pagerank circulant graph')\n",
    "print(\"nx:\\n\", change_nx_format(nx.pagerank_numpy(G_circle, alpha=alpha)))\n",
    "print(\"my:\\n\", my_pagerank(G_circle, alpha=alpha))\n",
    "print()\n",
    "\n",
    "G_path = nx.path_graph(3)\n",
    "print('Pagerank path graph')\n",
    "print(\"nx:\\n\", change_nx_format(nx.pagerank_numpy(G_path, alpha=alpha)))\n",
    "print(\"my:\\n\", my_pagerank(G_path, alpha=alpha, max_iter=1000))\n",
    "print()\n",
    "\n",
    "G_biparte = nx.DiGraph()\n",
    "G_biparte.add_edges_from([(1, 1), (2, 2), (3, 3), (4, 4), (5, 5), (1, 4), (1, 5), (2, 4), (2, 5), (3, 4), (3, 5)])\n",
    "print('Pagerank hubs and authorities graph')\n",
    "print(\"nx:\\n\", change_nx_format(nx.pagerank_numpy(G_biparte, alpha=alpha)))\n",
    "print(\"my:\\n\", my_pagerank(G_biparte, alpha=alpha, max_iter=1000))\n",
    "\n",
    "print() \n",
    "print(\"Matrix solve with tiny damping\")\n",
    "A = np.array(nx.adjacency_matrix(G_biparte).todense())\n",
    "W = (A / np.sum(A, 1, keepdims=True)).T\n",
    "\n",
    "d = 0.99\n",
    "n = G_biparte.order()\n",
    "\n",
    "print(\"damping factor: %.2f\" % d)\n",
    "P = np.linalg.inv(np.eye(n) - d * W) @ ((1-d) * np.ones((n, 1)) / n)\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Exercise 3 - HITS\n",
    " Your task is to implement your own version of the HITS algorithm.\n",
    " You may consult this [link](https://www.geeksforgeeks.org/hyperlink-induced-topic-search-hits-algorithm-using-networxx-module-python/).\n",
    " \n",
    " Apply your HITS algorithm on the karate network, to discover hubs and authorities.\n",
    " \n",
    " Make sure to normalize your matrix after every iteration, to ensure that values converge. (Be aware that nx normalizes differently from the link (using just the sum of hubs or authorities), so depending on how you implement normalization, you might not get the same result as nx.hits())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hits(G, k=50):\n",
    "    hubs = None\n",
    "    auths = None\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    return hubs, auths\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "G_test = G # G1\n",
    "t0 = time.time()\n",
    "h1, a1 = hits(G_test, k=50)\n",
    "t1 = time.time() - t0\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "h, a = nx.hits(G_test)\n",
    "t  = time.time() - t0\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if isinstance(h, dict):\n",
    "    h = [h[n] for n in G_test]\n",
    "    a = [a[n] for n in G_test]\n",
    "\n",
    "T = np.stack([h1, h, a1, a]).T\n",
    "\n",
    "if G_test.order() < 50: \n",
    "    nx.draw(G_test, pos=nx.circular_layout(G_test), labels={k: k for k in G_test.nodes})\n",
    "\n",
    "order = np.argsort(T[:,0])[::-1][:50]\n",
    "T1 = T[order,:2]\n",
    "\n",
    "order2 = np.argsort(T[:,2])[::-1][:50]\n",
    "T2 = T[order2,2:]\n",
    "T = np.concatenate([order.reshape(-1, 1), T1, order2.reshape(-1, 1), T2], axis=1)\n",
    "print(tabulate.tabulate(T, headers=['Node #', 'Hub1', 'Hub_nx', 'Node #', 'Aut1', 'Aut_nx']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = [nx.hits, hits]\n",
    "\n",
    "ns = [100, 200, 400, 800, 1600, 3200]\n",
    "\n",
    "graph_generators = [\n",
    "    lambda n: nx.fast_gnp_random_graph(n, 0.04),\n",
    "    lambda n: nx.gnm_random_graph(n, n*4, directed=True),\n",
    "    lambda n: nx.erdos_renyi_graph(n, 0.04, directed=True),\n",
    "    lambda n: nx.dual_barabasi_albert_graph(n, 10, 10, 0.4),\n",
    "]\n",
    "import time\n",
    "\n",
    "times = []\n",
    "from tqdm import tqdm\n",
    "for n in tqdm(ns):\n",
    "    graphs = [gen(n) for gen in graph_generators]\n",
    "    running_time = []\n",
    "    for algo in algorithms:\n",
    "        t = []\n",
    "        for g in graphs:\n",
    "            t0 = time.time()\n",
    "            h, a = algo(g)\n",
    "            t.append(time.time() - t0)\n",
    "        running_time.append(sum(t) / len(graphs))\n",
    "    times.append(running_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tabulate.tabulate(times))\n",
    "times = np.array(times)\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(ns, times[:,0], label=\"Ours (Edge)\")\n",
    "ax.plot(ns, times[:,1], label=\"NetworkX\")\n",
    "ax.set_xlabel(\"# Nodes\")\n",
    "ax.set_ylabel(\"Seconds\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 - HITS vs PageRank\n",
    "\n",
    "Download the `cit-DBLP` dataset from the [citation dataset collection](http://networkrepository.com/cit.php) in the [Network Repository](http://networkrepository.com/networks.php).\n",
    "\n",
    "1. Apply the PageRank and HITS algorithms on the `cit-DBLP` dataset.\n",
    "2. HITS hubs value, HITS authorities value, PageRank value. Calculate the [Kullback-Liebler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) and  [Jensen-Shannon divergence](https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence) between each pair of distribution. What do you observe?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "{\\displaystyle D_{\\text{KL}}(P\\parallel Q)=-\\sum _{x\\in {\\mathcal {X}}}P(x)\\log \\left({\\frac {Q(x)}{P(x)}}\\right)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "{D_{\\text{JS}}(P\\parallel Q)=\\frac{1}{2} D_{\\text{KL}}(P\\parallel M)+{\\frac {1}{2}}D_{\\text{KL}}(Q\\parallel M)}\\\\\n",
    "\\text{where }M = \\frac{1}{2}(P + Q)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method will load a list of all pairs of nodes that are \n",
    "# connected to each other by an edge.\n",
    "# Additionally, is will load precomputed positions for plotting nodes.\n",
    "# It is, however, not a pretty plot but the computation is fun (also plotting will take forever and potentially melt your pc)\n",
    "edges, pos = load_dblp_citations()\n",
    "print(\"Number of edges: \", len(edges))\n",
    "print(\"Number of nodes: \", len(pos))\n",
    "print(\"Position example: \", pos[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full cit-DBLP graph. Very large and time-consuming.\n",
    "G = nx.DiGraph()\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "# Small test graph ideal for testing while implementing\n",
    "G_test = nx.DiGraph()\n",
    "G_test.add_edges_from([(0, 3), (1, 2), (1, 4), (2, 0), \n",
    "                  (3, 2), (4, 3), (4, 1), (4, 5), \n",
    "                  (4, 2), (5, 2), (5, 7), (6, 0),  \n",
    "                  (6, 2), (7, 0)])\n",
    "# nx.draw(G_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl(p1, p2): \n",
    "    \n",
    "    ### TODO: Implement KL divergence\n",
    "    \n",
    "    return 0\n",
    "\n",
    "def js(p1, p2):\n",
    "    \n",
    "    ### TODO: Implement JS divergence\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compare authority, hubs and pagerank using kl and js divergence\n",
    "p = my_pagerank(G_test)\n",
    "h, a = hits(G_test)\n",
    "\n",
    "# p = my_pagerank(G)\n",
    "# h, a = hits(G)\n",
    "\n",
    "divergences = [\n",
    "    ['D_KL (P, A)', kl(p, a), 'D_KL(A, P)', kl(a, p), 'D_JS(A, P)', js(a, p)],\n",
    "    ['D_KL (P, H)', kl(p, h), 'D_KL(H, P)', kl(h, p), 'D_JS(H, P)', js(h, p)],\n",
    "    ['D_KL (H, A)', kl(h, a), 'D_KL(A, H)', kl(a, h), 'D_JS(A, H)', js(a, h)],\n",
    "]\n",
    "\n",
    "print(tabulate.tabulate(divergences))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5 - PageRank problems\n",
    "\n",
    "In this exercise, we study the effect of link farms on PageRank $\\alpha = 0.85$ and HITS\n",
    "\n",
    "1. Add a new node with an edge from and to the node with the lowest PageRank of the DBLP graph. Add 100 fake nodes with edges to and from this new node. Run PageRank and HITS and notice what changes. You may want to start with a smaller test-graph and fewer added nodes to speed up computation while implementing. \n",
    "2. Instead of the node with the lowest PageRank, connect the above node to the node with heighest PageRank - What do you notice? Why is that happening? \n",
    "3. Keep adding fake nodes until the new node has the highest PageRank and Authority score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# G = nx.DiGraph()\n",
    "# G.add_edges_from(edges)\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(10, 5))\n",
    "\n",
    "G = G_test.copy()\n",
    "nx.draw(G, with_labels=True, ax=ax[0])\n",
    "\n",
    "p = my_pagerank(G)\n",
    "\n",
    "print(p)\n",
    "\n",
    "lowest = np.argmin(p)\n",
    "highest = np.argmax(p)\n",
    "print(lowest, p[lowest])\n",
    "print(highest, p[highest])\n",
    "\n",
    "num_nodes = G.number_of_nodes()\n",
    "# print(num_nodes)\n",
    "\n",
    "\n",
    "print(\"Lowest Pagerank after adding 10 fake nodes\")\n",
    "## YOUR CODE TO ADD FAKE NODES\n",
    "## YOUR CODE TO ADD FAKE NODES\n",
    "    \n",
    "nx.draw(G, with_labels=True, ax=ax[1])\n",
    "\n",
    "p = nx.pagerank(G)\n",
    "\n",
    "print(lowest, p[lowest])\n",
    "\n",
    "print(\"Highest Pagerank after adding 10 fake nodes\")\n",
    "G = G_test.copy()\n",
    "\n",
    "## YOUR CODE TO ADD FAKE NODES\n",
    "## YOUR CODE TO ADD FAKE NODES\n",
    "\n",
    "nx.draw(G, with_labels=True, ax=ax[2])\n",
    "\n",
    "p = nx.pagerank(G)\n",
    "print(highest, p[highest])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
