{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 9 - Graph Embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week, we are going to work with graph embeddings. Let's first import the needed libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we have included some code to plot the karate graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_karate(G, pr=[], ax=None, cmap=plt.get_cmap('RdYlBu')): \n",
    "    fixed_positions = {0:(10.74,4.07),1:(9.76,6.48),2:(8.39,5.21),3:(10.37,1.98),4:(12.30,5.61),5:(13.31,3.28),6:(13.28,5.00),7:(8.41,7.06),8:(6.72,4.31),9:(5.77,1.38),10:(12.30,2.72),11:(12.75,4.05),12:(11.32,2.41),13:(8.70,2.88),14:(3.33,0.63),15:(1.88,2.01),16:(13.92,4.05),17:(10.77,5.61),18:(0.69,6.40),19:(9.05,1.38),20:(0.34,4.63),21:(11.56,6.22),22:(5.24,0.34),23:(1.88,7.49),24:(5.11,6.80),25:(4.31,8.52),26:(2.14,0.32),27:(3.65,6.64),28:(6.03,5.24),29:(0.77,2.91),30:(7.01,2.43),31:(6.61,7.86),32:(4.60,4.52),33:(4.39,2.91)}\n",
    "    if len(pr) :\n",
    "        nx.draw(G, with_labels=True, pos=fixed_positions, ax=ax, cmap=cmap, node_color=pr)\n",
    "    else : \n",
    "        nx.draw(G, with_labels=True, pos=fixed_positions, ax=ax)\n",
    "\n",
    "G = nx.karate_club_graph()\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "magic_cut = np.array([1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, -1, -1, 1, 1, -1, 1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1])\n",
    "\n",
    "plot_karate(G, magic_cut, ax=ax[0])\n",
    "plot_karate(G, np.random.rand(G.order()), ax=ax[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 - Theory \n",
    "\n",
    "We will first test ourselves on the theory of embeddings. Assume we have the (very simple) undirected line graph below. The graph is a line where node 1 is connected to node 2 and node 2 is connected to node 3. \n",
    "\n",
    "(1)-(2)-(3)\n",
    "\n",
    "1. Assume you have embedded the above graph with a **Linear Embedding** using the Adjacency matrix of the graph as the similarity matrix. How do you expect the embeddings to be if the embedding dimension is $d = 1$? Motivate your answer. \n",
    "2. Compute the linear embedding with $d=1$, showing all the steps. _Hint:_ Remember how to optimize linear embeddings! \n",
    "3. What happens to the linear embeddings with $d\\in\\{1,2,3\\}$ if we create a star graph, where node 3 is the central node and all the others are connected to node 3, and node 0 is connected with node 1? For simplicity assume that the graph has $5$ nodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - Similarity matrices\n",
    "In this exercise, we will implement different similarity matrices.\n",
    "\n",
    "For the following three node-similarity measures, implement the corresponding methods below.\n",
    " 1. Adjacency similarity.\n",
    " 1. 2-hop similarity ($A^2$).\n",
    " 1. Come up with your own measure of node similarity. You could, e.g., take neighbor-overlap into account. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjacency_similarity(G):\n",
    "    \"\"\"\n",
    "        Input: A networkx graph with n nodes\n",
    "        Output: A normlized adjacency matrix of size [n, n], where each row sum to 1\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE    \n",
    "\n",
    "    ### YOUR CODE HERE    \n",
    "\n",
    "\n",
    "def two_hub(G, k=2):\n",
    "    \"\"\"\n",
    "        Input: A networkx graph with n nodes\n",
    "        Output: A^2, where A is the adjacency matrix \n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE    \n",
    "\n",
    "    ### YOUR CODE HERE    \n",
    "\n",
    "\n",
    "def my_similarity(G):\n",
    "    \"\"\"\n",
    "        Input: A networkx graph with n nodes\n",
    "        Output: A [n, n] matrix of pair-wise similarities\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE    \n",
    "\n",
    "    ### YOUR CODE HERE    \n",
    "\n",
    "# # Check for normalization\n",
    "# fns = [adjacency_similarity,  two_hub, my_similarity]\n",
    "# fns = [adjacency_similarity,  my_similarity]\n",
    "# for fn in fns: \n",
    "#     assert np.allclose(fn(G)[0].sum(), 1)\n",
    "#     assert fn(G).sum() == G.order()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S1 = adjacency_similarity(G)\n",
    "S2 = two_hub(G)\n",
    "S3 = my_similarity(G)\n",
    "\n",
    "# Plot similaritites\n",
    "i = 2\n",
    "print(\"Plotting similarities for Node %d\" % i)\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "ax[0].set_title(\"Adjacency similarity\")\n",
    "plot_karate(G, S1[i,:], ax=ax[0], cmap=plt.get_cmap('Reds'))\n",
    "ax[1].set_title(\"Two hub similarity\")\n",
    "plot_karate(G, S2[i,:], ax=ax[1], cmap=plt.get_cmap('Blues'))\n",
    "ax[2].set_title(\"Your similarity\")\n",
    "plot_karate(G, S3[i,:], ax=ax[2], cmap=plt.get_cmap('Greens'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 - Linear embeddings\n",
    "\n",
    "This exercise is about Linear embeddings and how to perform dimensionality reduction on the similarity matrices above.\n",
    "\n",
    "Linear embeddings typically perform some linear dimensionality reduction on, variations of the Adjacency matrix $A\\in \\{0,1\\}^{n \\times n}$ for an **undirected graph** $G = (V,E)$. \n",
    "\n",
    "A linear embedding finds a matrix $Z \\in \\mathbb{R}^{n\\times d}$ which contains a $d$-dimensional representation of a given node in the network. The methods minimize a \"reconstruction error\" of the embedding with respect to the adjacecy matrix\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(A,Z) = \\sum_{(i,j) \\in E}(Z_{i}^\\top Z_{j} - A_{i,j})^2 = \\|ZZ^\\top - A\\|_F^2\n",
    "$$\n",
    "\n",
    "1. Show that if instead of one matrxix Z we have two $B \\in \\mathbb{R}^{n\\times d}$ and $C \\in \\mathbb{R}^{n\\times d}$, the best solution to the above minimization can be obtained by SVD on $A$\n",
    "2. What is the gradient matrix of the above function for a single row $Z_{i\\cdot}$ of the matrix $Z$?\n",
    "3. Implement the linear embedding above using gradient descent on the three different similaririty matrices from Exercise 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "def lin_grads(A, Z): \n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    ### YOUR CODE HERE    \n",
    "\n",
    "def lin_loss(A, Z): \n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "def learn(A, dim, grad = lin_grads, loss = lin_loss, step_size=0.001, steps=1000):\n",
    "    n, _ = A.shape\n",
    "    Z = np.random.rand(n, dim)\n",
    "    \n",
    "    ### YOUR CODE HERE    \n",
    "      \n",
    "    ### END CODE\n",
    "    return Z\n",
    "\n",
    "# Project the graph in 2D and plot it as points in the space\n",
    "Z1 = learn(S1, 2)\n",
    "Z2 = learn(S2, 2)\n",
    "Z3 = learn(S3, 2)\n",
    "\n",
    "# As colors to see what happens we can use the communities \n",
    "# that have been found with the \"magic method\" in the beginning.\n",
    "# A good embedding method should be able to separate the two colors! \n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "for i, (Z, T) in enumerate(zip([Z1, Z2, Z3], [\"Adjacency Matrix\", \"2-hub similarity\", \"My simlarity\"])): \n",
    "    ax[i].set_title(T)\n",
    "    ax[i].scatter(*(Z.T), c=magic_cut, cmap=plt.get_cmap('RdYlBu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 - VERSE\n",
    "\n",
    "In this exercise, we will try to implement and run one network embedding introduced in the lecture, called [VERSE](https://arxiv.org/pdf/1803.04742.pdf).\n",
    "\n",
    "VERSE is a simple *autoencoder* which takes in input a graph $G=(V,E)$, a row-normalized similarity matrix $S$ and learns an embedding matrix $Z \\in \\mathbb{R}^{n\\times d}$ using the following cross-entropy loss between the rows in $S$ and the rows in $ZZ^\\top$.\n",
    "\n",
    "VERSE loss function for each node $i$ is \n",
    "$$\n",
    "\\mathcal{L}(S_i,Z) = -S_i \\log(\\text{softmax}(Z_iZ^\\top)) \n",
    "$$\n",
    "$$\n",
    "\\log(\\text{softmax}(Z_iZ^\\top))  = [\\log\\frac{e^{Z_iZ_0^\\top}}{\\sum_je^{Z_iZ_j^\\top}},\\log\\frac{e^{Z_iZ_1^\\top}}{\\sum_je^{Z_iZ_j^\\top}},...,\\log\\frac{e^{Z_iZ_{n-1}^\\top}}{\\sum_je^{Z_iZ_j^\\top}}]\n",
    "$$\n",
    "$$\n",
    "-S_i\\log(\\text{softmax}(Z_iZ^\\top))   = -({S_i}_0\\log\\frac{e^{Z_iZ_0^\\top}}{\\sum_je^{Z_iZ_j^\\top}}+{S_i}_1\\log\\frac{e^{Z_iZ_1^\\top}}{\\sum_je^{Z_iZ_j^\\top}}+...+{S_i}_{n-1}\\log\\frac{e^{Z_iZ_{n-1}^\\top}}{\\sum_je^{Z_iZ_j^\\top}})\n",
    "$$\n",
    "\n",
    "which can be written in terms of the global loss\n",
    "$$\n",
    "\\mathcal{L}(S,Z) = \\sum_{i \\in V}\\mathcal{L}(S_i,Z)\n",
    "$$\n",
    "here $\\text{softmax}(\\cdot)$ is the [softmax](https://en.wikipedia.org/wiki/Softmax_function) function. \n",
    "\n",
    "The similarity matrix in case of VERSE is typically the Personalized PageRank (PPR) matrix. For the scope of the exercise we will use the the PageRank algorithm with $\\alpha = 0.85$.\n",
    "\n",
    "\n",
    "\n",
    "We will implement the full VERSE method and understand how it works in practice \n",
    "1. Compute the PageRank algorithm with $\\alpha = 0.85$. \n",
    "2. Implement the VERSE Loss function\n",
    "3. Compute the gradient of the loss function. **Hint: do it row-wise (i.e., node-by-node)**\n",
    "4. Try to change the parameters and use another similarity function. What other similarity function among nodes can you use? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "def pagerank_matrix(G, alpha = 0.85) :     \n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "G = nx.karate_club_graph()\n",
    "P = pagerank_matrix(G)\n",
    "\n",
    "# Sanity checks on the matrix\n",
    "n = G.number_of_nodes()\n",
    "per = np.zeros(n)\n",
    "per[2] = 1\n",
    "\n",
    "# assert np.allclose(my_pagerank(G,r=per), P[2,:], atol=1e-3, rtol=1e-3), \"%s\\n%s\" % (str(my_pagerank(G, r=per)), str(P[2]))\n",
    "# assert np.allclose(P[1,:].sum(), 1.)\n",
    "# End sanity check.\n",
    "\n",
    "print(pagerank_matrix(G))\n",
    "print(nx.pagerank(G, personalization={i: 1 if i == 0 else 0 for i in range(n)}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verse_loss(S, Z): \n",
    "    loss = 0\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    return loss\n",
    "\n",
    "def verse_grads(S, Z): \n",
    "    grads = np.zeros(Z.shape)\n",
    "    ### YOUR CODE HERE \n",
    "\n",
    "    ### YOUR CODE HERE \n",
    "    return grads\n",
    "\n",
    "Z1 = learn(P, 2, grad=verse_grads, loss=verse_loss, step_size = 0.005, steps=4000)\n",
    "Z2 = learn(S3, 2, grad=verse_grads, loss=verse_loss, step_size = 0.005, steps=4000)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].scatter(np.array(Z1[:,0]), np.array(Z1[:,1]), c=magic_cut)\n",
    "ax[0].set_title(\"Page Rank\")\n",
    "ax[1].scatter(np.array(Z2[:,0]), np.array(Z2[:,1]), c=magic_cut)\n",
    "ax[1].set_title(\"My similarity\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
